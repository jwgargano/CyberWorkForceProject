{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b786c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this for first install of packages!\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d31ff77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import helpers # this is a Joe G. created helper file of functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bb5587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params / Files to change \n",
    "input_file = '../data/cleaned_data/USAJobs.csv' # change to whatever file/filepath you are using\n",
    "output_file = '../data/results/usa_jobs/similarity_match/batch1.csv' # change to your outpath\n",
    "desc_column = 'Duties'\n",
    "start_idx = 0 # file row to start at\n",
    "end_idx = 100 # file row to end at\n",
    "#how many jobs do you want to search / score against? make start_idx -1 if you want to use entire file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15558e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Pre-trained model \n",
    "# downloads automatically from hugging face\n",
    "model_name = 'sentence-transformers/paraphrase-distilroberta-base-v2'\n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134c6d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file for our cyber baseline\n",
    "# hopefully can use embedding file later\n",
    "# ? Is Knowledge_Units.csv the final baseline file? \n",
    "baseline_file = '../data/cleaned_data/KUKSAT_Baseline.csv'\n",
    "ksa_col = 'KUKSAT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad862d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read baselne_file and convert to df\n",
    "df_base = pd.read_csv(baseline_file, encoding='ISO-8859-1')\n",
    "ksas = df_base[ksa_col].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf180aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many ksas\n",
    "len(df_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d6c204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read input file and covert to df\n",
    "# covert target description column to list\n",
    "df = pd.read_csv(input_file)\n",
    "if start_idx >= 0 and start_idx != end_idx:\n",
    "    jobs = df[start_idx:end_idx]\n",
    "else:\n",
    "    jobs = df.copy()\n",
    "jobs = jobs[desc_column].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2059397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many jobs\n",
    "len(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721415f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply text cleanup functions to jobs and ksa base lists\n",
    "jobs = helpers.cleanup_text(jobs)\n",
    "ksas = helpers.cleanup_text(ksas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16343dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split jobs to sentence level\n",
    "job_sent = helpers.split_sents(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb9a63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets vector embeddings for the ksa baselines\n",
    "baseline_vecs = model.encode(ksas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2df11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save baseline embeddings uncomment here.\n",
    "helpers.save_embeddings(baseline_vecs, '../data/saved_embeddings/baseline.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb0c77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode all job sentences\n",
    "all_job_vecs = [model.encode(sent) for sent in job_sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b7e430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save job sentence embeddings\n",
    "helpers.save_embeddings(baseline_vecs, '../data/saved_embeddings/usajob_test.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04e9f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try using each KSA and compare it to each sentence for each job description\n",
    "# Find sentences that matches (x >= 0.6), similar matches ( 0.6< x >0.4), missing ( x < 0.4)\n",
    "all_matches = []\n",
    "all_similar =[]\n",
    "all_missing =[]\n",
    "# Loops through baseline vectors embeddings of the ksas\n",
    "for idx, ksa in enumerate(baseline_vecs):\n",
    "    print('ksa', idx, '_'*10)\n",
    "    matched=[]\n",
    "    similar=[]\n",
    "    missing=[]\n",
    "#   loops through all the job sentence vector embeddings\n",
    "    for idx2, sent in enumerate(all_job_vecs):\n",
    "        print('..job ', idx2)\n",
    "#         evaluates the cosine similarity between the ksa and the job sentences\n",
    "        val = cosine_similarity([ksa], sent)\n",
    "    \n",
    "#     Loops through the scores to determine if matched, similar, or missing\n",
    "        for idx3, num in enumerate(val[0]):\n",
    "#         created a temp dictionarty of key info and then appends that to the appropriate list\n",
    "            temp = {}\n",
    "#     if ksa text in sentence text then match too\n",
    "            if num >=0.6 or (ksas[idx].lower().lstrip().strip() in job_sent[idx2][idx3].lower().lstrip().strip()):\n",
    "                temp['job_idx'] = idx2\n",
    "                temp['sentence_idx'] = idx3\n",
    "                temp['ksa_idx'] = idx\n",
    "                temp['sentence_text'] = job_sent[idx2][idx3]\n",
    "                temp['ksa_text'] = ksas[idx]\n",
    "                temp['sim_score'] = num\n",
    "                matched.append(temp)\n",
    "            elif num < 0.6 and num > 0.4:\n",
    "                temp['job_idx'] = idx2\n",
    "                temp['sentence_idx'] = idx3\n",
    "                temp['ksa_idx'] = idx\n",
    "                temp['sentence_text'] = job_sent[idx2][idx3]\n",
    "                temp['ksa_text'] = ksas[idx]\n",
    "                temp['sim_score'] = num\n",
    "                similar.append(temp)\n",
    "            else:\n",
    "                temp['job_idx'] = idx2\n",
    "                temp['sentence_idx'] = idx3\n",
    "                temp['ksa_idx'] = idx\n",
    "                temp['sentence_text'] = job_sent[idx2][idx3]\n",
    "                temp['ksa_text'] = ksas[idx]\n",
    "                temp['sim_score'] = num\n",
    "                missing.append(temp)\n",
    "    all_matches.append(matched)\n",
    "    all_similar.append(similar)\n",
    "    all_missing.append(missing)\n",
    "    print('**\\n**')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651f3e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates total sentences compared \n",
    "sent_total = 0\n",
    "for x in job_sent:\n",
    "    sent_total += len(x)\n",
    "print(sent_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61742cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates aggregate scores for matching, similar, missing\n",
    "# uses sentence total \n",
    "# ? OR should we do just len(job) ?\n",
    "ksa_agg_matches = []\n",
    "ksa_agg_similar = []\n",
    "ksa_agg_missing = []\n",
    "for idx, val in enumerate(baseline_vecs):\n",
    "    print('ksa', idx, '_'*10)\n",
    "    matched_score = len(all_matches[idx])/sent_total\n",
    "    similar_score = len(all_similar[idx])/sent_total\n",
    "    missing_score = 1 -(matched_score + similar_score)\n",
    "    ksa_agg_matches.append(matched_score)\n",
    "    ksa_agg_similar.append(similar_score)\n",
    "    ksa_agg_missing.append(missing_score)\n",
    "    print('Match Score ==', matched_score)\n",
    "    print('Similar Score == ', similar_score)\n",
    "    print('Missing Score ==', missing_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8a10b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframes all results\n",
    "final_df = pd.DataFrame({\n",
    "    'ksa': ksas,\n",
    "    'matches': all_matches,\n",
    "    'similar': all_similar,\n",
    "    'missing': all_missing,\n",
    "    'matched_score': ksa_agg_matches,\n",
    "    'similar_score': ksa_agg_similar,\n",
    "    'missing_score': ksa_agg_missing\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2caf3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to output_file\n",
    "final_df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71a1134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For local visual / spot check sorts by matched, similar, missing score\n",
    "final_df.sort_values(by=['matched_score', 'similar_score', 'missing_score'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8923b442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For local visual / spot check prints ksa's 1 matches\n",
    "for idx, row in final_df.iterrows():\n",
    "    print('ksas===', row['ksa'], ' \\n ')\n",
    "    for match in row['matches']:\n",
    "        print('matched ===', match['sentence_text'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b82abd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
